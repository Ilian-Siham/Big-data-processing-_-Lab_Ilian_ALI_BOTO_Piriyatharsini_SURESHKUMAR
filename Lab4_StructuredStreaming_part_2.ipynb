{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccc7944f-33fc-40fc-803f-9e5139c5c7bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    BooleanType, LongType, MapType\n",
    ")\n",
    "from pyspark.sql.functions import window, col\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6d0e01-085f-4f26-8f02-0c172986977b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# project data catalog defined and created in <placeholder> notebook\n",
    "catalog = 'wikimedia_db'\n",
    "\n",
    "# db_schema containing unprocessed/streaming data\n",
    "uc_schema_raw_events = 'raw_events'\n",
    "\n",
    "# raw data is saved in a temp volume by yy_mm_day\n",
    "raw_events_volume_time = datetime.now()\n",
    "raw_events_volume =  f\"events_tmp_{raw_events_volume_time.strftime('%y_%m_%d')}\"\n",
    "raw_data_path = f'/Volumes/{catalog}/{uc_schema_raw_events}/{raw_events_volume}'\n",
    "\n",
    "# db schema for checkpointing streaming tables\n",
    "db_schema_checkpoints = 'checkpoints'\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.cleaned_events\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.gold_events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052de597-ddb8-4ff4-9693-57b6706c5156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Meta schema (nested)\n",
    "meta_schema = StructType([\n",
    "    StructField(\"uri\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"dt\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"stream\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Length schema (nested)\n",
    "length_schema = StructType([\n",
    "    StructField(\"old\", IntegerType(), True),\n",
    "    StructField(\"new\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Revision schema (nested)\n",
    "revision_schema = StructType([\n",
    "    StructField(\"old\", LongType(), True),\n",
    "    StructField(\"new\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Main recent change schema\n",
    "recentchange_schema = StructType([\n",
    "    StructField(\"$schema\", StringType(), True),\n",
    "    StructField(\"meta\", meta_schema, True),\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"namespace\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"bot\", BooleanType(), True),\n",
    "    StructField(\"minor\", BooleanType(), True),\n",
    "    StructField(\"patrolled\", BooleanType(), True),\n",
    "    StructField(\"length\", length_schema, True),\n",
    "    StructField(\"revision\", revision_schema, True),\n",
    "    StructField(\"server_url\", StringType(), True),\n",
    "    StructField(\"server_name\", StringType(), True),\n",
    "    StructField(\"wiki\", StringType(), True),\n",
    "    StructField(\"parsedcomment\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a940eea-2d4d-431c-ad2e-d9cedcb068d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from a file\n",
    "# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\n",
    "streamingInputDF = (\n",
    "  spark\n",
    "    .readStream                       \n",
    "    .schema(recentchange_schema)               # Set the schema of the JSON data\n",
    "    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking n number of files at a time\n",
    "    .json(raw_data_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5a91d7-2966-40e3-9c0a-52bb5f439221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Do some transformations\n",
    "# Same query as staticInputDF\n",
    "streamingCountsDF = (\n",
    "  streamingInputDF\n",
    "    .groupBy(\n",
    "      streamingInputDF.bot, # group by edit made by bot boolean\n",
    "      window(\n",
    "        col(\"timestamp\").cast(\"timestamp\"), \n",
    "        \"5 minutes\"\n",
    "      )\n",
    "    )\n",
    "    .count()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3d6e74f-1e88-4802-8b15-0488d35abfdf",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"server_url\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762263415711}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# temp volume for checkpoint storage\n",
    "volume = 'tmp_streamingInputDF'\n",
    "volume_path = f'/Volumes/{catalog}/{db_schema_checkpoints}/{volume}'\n",
    "volume_name = f'{catalog}.{db_schema_checkpoints}.{volume}'\n",
    "\n",
    "# drop old temp volume and recreate\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {volume_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "\n",
    "# Display the streaming dataframe\n",
    "streamingInputDF.display(checkpointLocation=volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02469fad-4c49-4a15-9fcb-adbd423d0b1f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"window\":545},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761928564876}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# temp volume for checkpoint storage\n",
    "volume = 'tmp_streamingDF'\n",
    "volume_path = f'/Volumes/{catalog}/{db_schema_checkpoints}/{volume}'\n",
    "volume_name = f'{catalog}.{db_schema_checkpoints}.{volume}'\n",
    "\n",
    "# drop old temp volume and recreate\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {volume_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "\n",
    "# Display transformed data\n",
    "streamingCountsDF.display(checkpointLocation=volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56030828-6e71-4f8d-a590-c91c8a537039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alerting System for Rare Events\n",
    "\n",
    "# Événement rare = modification faite par un bot\n",
    "rareEventsDF = streamingInputDF.filter(col(\"bot\") == True)\n",
    "\n",
    "# Create schema for alerts \n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.alerts\")\n",
    "\n",
    "# Create volume for alerts\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.alerts.bot_events\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.alerts.checkpoints\")\n",
    "\n",
    "# Create volume path for alerts\n",
    "alerts_volume_path = f\"/Volumes/{catalog}/alerts/bot_events\"\n",
    "alerts_checkpoint_path = f\"/Volumes/{catalog}/alerts/checkpoints\"\n",
    "\n",
    "# Stream rare events (append mode)\n",
    "rareEventsDF.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", alerts_checkpoint_path) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(alerts_volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "931db4d7-19d2-4bfa-9782-808e07ee0ae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean-up Job\n",
    "\n",
    "print(f\"Scanning raw event volumes in schema: {catalog}.{uc_schema_raw_events}\")\n",
    "\n",
    "# 1. Get list of volumes\n",
    "volumes_df = spark.sql(f\"SHOW VOLUMES IN {catalog}.{uc_schema_raw_events}\")\n",
    "\n",
    "volumes_df.show()   \n",
    "\n",
    "# 2. Loop through returned volumes\n",
    "for row in volumes_df.collect():\n",
    "    volume_name = row['volume_name']   \n",
    "\n",
    "    if \"events_tmp\" in volume_name:\n",
    "        full_path = f\"/Volumes/{catalog}/{uc_schema_raw_events}/{volume_name}\"\n",
    "        print(f\"Deleting old raw volume: {full_path}\")\n",
    "        dbutils.fs.rm(full_path, recurse=True)\n",
    "\n",
    "print(\"Clean-up job finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7348899765119211,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Lab4_StructuredStreaming_part_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
