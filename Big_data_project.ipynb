{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db1d2163-f035-46e7-8a46-ffe6008bf768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Big Data Processing Project #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0276b25-ba3a-42ba-8b3c-eaefa3933b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Project Part I: Batch Processing ##\n",
    "**Q.1.** Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f778b2be-a77e-4b42-8926-420184d33cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the name of the new catalogue, schema & volume\n",
    "catalog = \"Big_data_project\"\n",
    "schema = \"IMDB_schema\"\n",
    "volume = \"IMDB_volume\"\n",
    "# Create the new schema & volume\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")\n",
    "\n",
    "# Download data from website\n",
    "IMDb_URL = \"https://datasets.imdbws.com/\"\n",
    "data_name = [\"name.basics.tsv.gz\", \"title.akas.tsv.gz\", \"title.basics.tsv.gz\", \"title.crew.tsv.gz\", \"title.episode.tsv.gz\", \"title.principals.tsv.gz\", \"title.ratings.tsv.gz\"]\n",
    "for name in data_name:\n",
    "  data_url = f\"{IMDb_URL}{name}\"\n",
    "  target_path = f\"/Volumes/{catalog}/{schema}/{volume}/{name}\"\n",
    "  dbutils.fs.cp(data_url, target_path)\n",
    "  print(f\"Data {name} downloaded to {target_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f07450a-094d-4d65-920b-5837e2708a47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data into a spark dataframe\n",
    "df_name = spark.read.format('csv')\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "    .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[0]}\")\n",
    "df_title = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[1]}\")\n",
    "df_basic = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[2]}\")\n",
    "df_crew = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[3]}\")\n",
    "df_episode = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[4]}\")\n",
    "df_principals = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .option(\"nullValue\", \"\\\\N\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[5]}\")\n",
    "df_ratings = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "        .load(f\"/Volumes/{catalog}/{schema}/{volume}/{data_name[6]}\")\n",
    "\n",
    "df_name.limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6507d4e-a656-4468-8247-27b631581db0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_name.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b1d2ca-3c06-4202-9b39-9bb46be7efb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_name = (df_name\n",
    "    .withColumn('birthYear', col('birthYear').cast('integer'))\n",
    "    .withColumn('deathYear', col('deathYear').cast('integer'))\n",
    ")\n",
    "\n",
    "# Check the schema to confirm the change\n",
    "df_name.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c04136d-2446-44ee-9e95-53b0afb5b24e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.2.** How many total people in data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd022750-bb8f-434d-9862-325c3721a3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Number of rows in name table\n",
    "nbr_prs = df_name.select('nconst').distinct().count()\n",
    "print(f\"There are {nbr_prs} people in the data set. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ef3a07-8681-4291-b972-ff6365539e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.3.** What is the earliest year of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a42bac2a-214d-4a20-90df-30574f2f9f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the earliest year of birth \n",
    "df_name.select('birthYear').filter(col(\"birthYear\").isNotNull()).orderBy('birthYear', ascending=False).limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b88ab0-01a1-4432-acbe-8379692dc7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The earliest year of birth is 2025\n",
    "\n",
    "**Q.4.** How many years ago was this person born?\n",
    "\n",
    "This person was born this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fc95e2-8a7c-4bc6-87b4-d4e8ca752e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.5.** Using only the data in the data set, determine if this date of birth correct.\n",
    "\n",
    "No the birth date is not correct.\n",
    "\n",
    "**Q.6.** Explain the reasoning for the answer in a code comment or new markdown cell.\n",
    "\n",
    "The birth date is not correct because this person is supposedly born this year but she already have a profession. This is not logical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b44673-328c-4cd9-ab1c-54adad85f56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_name.filter(col(\"birthYear\").isNotNull()).orderBy('birthYear', ascending=False).limit(2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb7fef34-f26c-4896-a3c6-e81312b22917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.7.** What is the most recent date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe56dfe-e972-406d-814f-e478d6b81259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Find the most recent data of birth\n",
    "print(\"Dataframe result : \")\n",
    "df_name.filter(col('birthYear').isNotNull()).filter(col(\"birthYear\")< 2025).orderBy('birthYear', ascending=False).limit(2).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e13bb326-5f4d-4d56-99b1-e5147fd8cf6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The most recent date of birth is 2024\n",
    "\n",
    "**Q.8.** What percentage of the people do not have a listed date of birth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dca7bb4d-5059-4c23-b161-60ace7865b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "per_of_no_birth = df_name.filter(col(\"birthYear\").isNull()).count() / df_name.count()\n",
    "print(f\"{per_of_no_birth*100:.2f}% of people have no birth year.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ce11c3-c16c-46cd-8f37-3382e1b0b428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.9.** What is the length of the longest \"short\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c7d658-694b-4de6-bde0-6f40152ce437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the appropriate data type\n",
    "df_basic = (df_basic.withColumn('endYear', col('endYear').cast('integer')) \\\n",
    "    .withColumn('startYear', col('startYear').cast('integer')) \\\n",
    "    .withColumn('runtimeMinutes', col('runtimeMinutes').cast('integer')) \\\n",
    "    .withColumn('isAdult', col('isAdult').cast('boolean')))\n",
    "\n",
    "df_basic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce850212-4c59-4372-8c65-5e71df392d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the longest run time \"short\"\n",
    "longest_short = df_basic.filter(col(\"runtimeMinutes\").isNotNull()) \\\n",
    "    .filter(col(\"titleType\")=='short') \\\n",
    "    .filter(col(\"startYear\")>1900) \\\n",
    "    .orderBy('runtimeMinutes', ascending=False).limit(1)\n",
    "\n",
    "print(\"DataFrame result:\")\n",
    "longest_short.show() \n",
    "\n",
    "# Collect runtime value\n",
    "longest_short_row = longest_short.collect()[0]\n",
    "runtime = longest_short_row['runtimeMinutes']\n",
    "print(f\"The length of the longest short film after 1900 is {runtime} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cedd547-3574-4586-a8c7-6d8bf20e5002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Q.10.** What is the length of the shortest \"movie\" after 1900?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32e2da7-75b1-4e5b-a308-467be99f64f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the shortest movie after 1900\n",
    "shortest_movie = df_basic.filter(col(\"runtimeMinutes\").isNotNull()) \\\n",
    "    .filter(col(\"titleType\")=='movie') \\\n",
    "    .filter(col(\"startYear\")>1900) \\\n",
    "    .orderBy('runtimeMinutes', ascending=True).limit(1)\n",
    "\n",
    "print(\"DataFrame result:\")\n",
    "shortest_movie.show()\n",
    "\n",
    "# collect the runtime value\n",
    "shortest_movie_row = shortest_movie.collect()[0]\n",
    "runtime = shortest_movie_row['runtimeMinutes']\n",
    "print(f\"The length of the shortest movie after 1900 is {runtime} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ca21eb-38c5-4724-a5ab-d114cb0f2a5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.11.** List of all of the genres represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9827cd-072e-4af1-990f-5fb10bd6b9aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Collect the list of the genres represented\n",
    "genres = df_basic.select('genres').distinct().collect()\n",
    "print(f\"Those are the genres represented in the data set {genres}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b23a8e59-87a6-427c-af66-81ab19cc9c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.12.** What is the highest rated comedy \"movie\" in the dataset? Note, if there is a tie, the tie shall be broken by the movie with the most votes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0201c9-5e73-4dbf-97f4-046ca981061d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ratings.limit(5).display()\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "967ad0f7-6304-4f07-8953-d7e59b046d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Change the data type of df_rating into the appropriate data type\n",
    "df_ratings = df_ratings.withColumn('averageRating', col('averageRating').cast('float')) \\\n",
    "    .withColumn('numVotes', col('numVotes').cast('integer'))\n",
    "\n",
    "df_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84880d1-b7a3-47e9-8ed8-759dc10a0c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the highest rated \"comedy\" movie\n",
    "highest_rated_comedy = df_basic.join(df_ratings, df_basic.tconst == df_ratings.tconst) \\\n",
    "    .filter(col(\"titleType\")=='movie') \\\n",
    "    .filter(col(\"genres\").contains('Comedy')) \\\n",
    "    .orderBy(['averageRating', 'numVotes'], ascending=False).limit(5)\n",
    "\n",
    "print(\"DataFrame result:\")\n",
    "display(highest_rated_comedy)\n",
    "\n",
    "# Collect the names of the movies\n",
    "highest_rated_comedy1 = highest_rated_comedy.collect()[0]\n",
    "highest_rated_comedy2 = highest_rated_comedy.collect()[1]\n",
    "print(f\"The highest rated comedy movies, with the same score and the same number of votes are : {highest_rated_comedy1['primaryTitle']} and {highest_rated_comedy2['primaryTitle']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "812b6a08-3294-4cac-b349-f0676c46fa40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.13.** Who was the director of the movie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0863e6b4-aaef-472c-a229-4cff19bf538e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the directors of those movies\n",
    "highest_rated_comedyM = highest_rated_comedy.join(df_crew, on='tconst', how='inner') \\\n",
    "    .filter(col(\"directors\").isNotNull()) \\\n",
    "    .orderBy([\"averageRating\",\"numVotes\"], ascending=False)\n",
    "\n",
    "print(\"DataFrame result:\")\n",
    "print(\"Highest rated comedy\")\n",
    "highest_rated_comedyM.display()\n",
    "\n",
    "director1 = highest_rated_comedyM.collect()[0]\n",
    "director2 = highest_rated_comedyM.collect()[1]\n",
    "\n",
    "# Get the director name with the collected id\n",
    "director_name1 = df_name.filter(col(\"nconst\")==director1['directors']) \\\n",
    "    .select('primaryName')\n",
    "director_name2 = df_name.filter(col(\"nconst\")==director2['directors']) \\\n",
    "    .select('primaryName')\n",
    "\n",
    "print(\"Directors names\")\n",
    "director_name1.display()\n",
    "director_name2.display()\n",
    "\n",
    "print(f'The director of the highest rated comedy movie \"O La La\" is {director_name1.collect()[0]['primaryName']}')\n",
    "print(f'The  other director of the second highest rated comedy movie(same score and same number of votes), \"Space Melody\", is {director_name2.collect()[0]['primaryName']}')\n",
    "\n",
    "#print(f\"The directors of the highest rated comedy movies are : {director1['directors']} and {director12['directors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2458693d-c875-4fbe-bacf-8810f4ecc5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q.14.** List, if any, the alternate titles for the movie.\n",
    "\n",
    "Those highest rated comedy movie doesn't have any alternate title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d369c0a7-0343-4d5c-8d9b-02ce81835959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Project Part II: Streaming Processing ##\n",
    "\n",
    "1. Data Ingestion (Prerequisite)\n",
    "\n",
    "Before running this analysis, you must execute the separate ingestion notebook: wikimedia_get_raw_stream_data.\n",
    "\n",
    "**Purpose**: This notebook establishes a connection to the Wikimedia EventStreams API.\n",
    "\n",
    "**Function**: It captures live JSON events for all English Wikipedia edits and persists them into a Unity Catalog Volume as raw files.\n",
    "\n",
    "**Action**: Ensure that the ingestion notebook has been running for at least 10–15 minutes to provide a sufficient data sample for the filters.\n",
    "\n",
    "2. Stream Processing & Filtering\n",
    "\n",
    "Once the raw data is available, this notebook performs the following processing steps:\n",
    "\n",
    "**Schema Definition**: We apply a strict StructType schema to the raw JSON to ensure data integrity during the readStream process.\n",
    "\n",
    "**Entity Selection**: We define five specific entities chosen randomly (e.g., Drama, Star Wars, Johnny Depp, Game of Thrones, and Thriller) and some entities extracted from the IMDb dataset (e.g., Comedy movie) to track within the global edit stream.\n",
    "\n",
    "**Filtering Logic**: We use Regex (rlike) and namespace filtering to isolate edits belonging to our entities while excluding \"noise\" (such as internal Wikipedia talk pages or category maintenance).\n",
    "\n",
    "3. Metrics and Alerting\n",
    "\n",
    "The structured stream is split into two distinct outputs (Sinks):\n",
    "\n",
    "**Simple Metrics**: A windowed aggregation that counts the number of edits per entity  over a 10-minute period, stored in a Delta Table.\n",
    "\n",
    "**Alerting System**: A secondary stream that filters for specific event types (e.g., automated Bot edits). These \"alerts\" are routed to a separate destination to mimic a real-world notification or auditing system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29bc1cac-1433-4401-b8a9-ecbf505b2ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Get Raw stream data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5ed60e33-5a48-4343-9a6c-b95ba37e754f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    BooleanType, LongType, MapType\n",
    ")\n",
    "from pyspark.sql.functions import window, col, when, count, lit\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f5a7f1-5b35-464f-b25d-203111aa6a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# project data catalog defined and created in <placeholder> notebook\n",
    "catalog = 'wikimedia_db'\n",
    "\n",
    "# db_schema containing unprocessed/streaming data\n",
    "uc_schema_raw_events = 'raw_events'\n",
    "\n",
    "# raw data is saved in a temp volume by yy_mm_day\n",
    "raw_events_volume_time = datetime.now()\n",
    "raw_events_volume =  f\"events_tmp_{raw_events_volume_time.strftime('%y_%m_%d')}\"\n",
    "raw_data_path = f'/Volumes/{catalog}/{uc_schema_raw_events}/{raw_events_volume}'\n",
    "\n",
    "# db schema for checkpointing streaming tables\n",
    "db_schema_checkpoints = 'checkpoints'\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{db_schema_checkpoints}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.cleaned_events\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.gold_events\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd33a7a0-e44c-48a9-a22f-f2f31a6d0582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Meta schema (nested)\n",
    "meta_schema = StructType([\n",
    "    StructField(\"uri\", StringType(), True),\n",
    "    StructField(\"request_id\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"dt\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"stream\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Length schema (nested)\n",
    "length_schema = StructType([\n",
    "    StructField(\"old\", IntegerType(), True),\n",
    "    StructField(\"new\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Revision schema (nested)\n",
    "revision_schema = StructType([\n",
    "    StructField(\"old\", LongType(), True),\n",
    "    StructField(\"new\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Main recent change schema\n",
    "recentchange_schema = StructType([\n",
    "    StructField(\"$schema\", StringType(), True),\n",
    "    StructField(\"meta\", meta_schema, True),\n",
    "    StructField(\"id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"namespace\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"comment\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"bot\", BooleanType(), True),\n",
    "    StructField(\"minor\", BooleanType(), True),\n",
    "    StructField(\"patrolled\", BooleanType(), True),\n",
    "    StructField(\"length\", length_schema, True),\n",
    "    StructField(\"revision\", revision_schema, True),\n",
    "    StructField(\"server_url\", StringType(), True),\n",
    "    StructField(\"server_name\", StringType(), True),\n",
    "    StructField(\"wiki\", StringType(), True),\n",
    "    StructField(\"parsedcomment\", StringType(), True),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9a3d4b-fa9c-49eb-a2e3-47ae2688b24b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from a file\n",
    "streamingInputDF = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .schema(recentchange_schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(raw_data_path)     # now dynamically resolved\n",
    ")\n",
    "\n",
    "# temp volume for checkpoint storage\n",
    "volume = 'tmp_streamingInputDF'\n",
    "volume_path = f'/Volumes/{catalog}/{db_schema_checkpoints}/{volume}'\n",
    "volume_name = f'{catalog}.{db_schema_checkpoints}.{volume}'\n",
    "\n",
    "# drop old temp volume and recreate\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {volume_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "\n",
    "# Display the streaming dataframe\n",
    "streamingInputDF.display(checkpointLocation=volume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56cb5bb8-dd1e-47f2-8d6a-63f8cb5c6fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Get your entities ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d29eb2-f84d-4bf4-820f-13620e10e9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window\n",
    "\n",
    "# 1. Define 5 High-Activity Entities\n",
    "# We use simple keywords to ensure we catch many related pages\n",
    "project_entities = [\"Drama\", \"Star_Wars\", \"Johnny_Depp\", \"Game_of_Thrones\", \"Thriller\"]\n",
    "entity_pattern = \"(?i)\" + \"|\".join(project_entities)\n",
    "\n",
    "# 2. Filter the stream (Strictly removing non-article noise)\n",
    "df_active_entities = streamingInputDF.filter(\n",
    "    ~col(\"title\").contains(\":\") &           # Keeps only main articles\n",
    "    col(\"title\").rlike(entity_pattern)       # Matches any of our 5 entities\n",
    ")\n",
    "\n",
    "# 3. Simple Metric: Count edits per entity every 10 minutes\n",
    "streamingCountDF = (df_active_entities\n",
    "    .groupBy(\n",
    "        \"title\", \n",
    "        window(col(\"timestamp\").cast(\"timestamp\"), \"10 minutes\")\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "\n",
    "streamingCountbotDF = (df_active_entities\n",
    "                       .groupBy(\"bot\",\n",
    "                                window(col(\"timestamp\").cast(\"timestamp\"), \"10 minutes\"))\n",
    "                       .count())\n",
    "\n",
    "\n",
    "# 4. Alert Metric: Route Bot Edits to a different file\n",
    "df_alerts = df_active_entities.filter(col(\"bot\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d345ec67-a1d2-4cc6-a057-b539c24422ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "# temp volume for checkpoint storage\n",
    "volume = 'final_project_streamingDF'\n",
    "volume_path = f'/Volumes/{catalog}/{db_schema_checkpoints}/{volume}'\n",
    "volume_name = f'{catalog}.{db_schema_checkpoints}.{volume}'\n",
    "\n",
    "# drop old temp volume and recreate\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {volume_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "\n",
    "# Define sub-paths\n",
    "output_title_path = f\"{volume_path}/data_title\"\n",
    "checkpoint_title_path = f\"{volume_path}/checkpoints_title\"\n",
    "\n",
    "output_bot_path = f\"{volume_path}/data_bot\"\n",
    "checkpoint_bot_path = f\"{volume_path}/checkpoints_bot\"\n",
    "\n",
    "output_path = f\"{volume_path}/data\"\n",
    "checkpoint_path = f\"{volume_path}/checkpoints\"\n",
    "\n",
    "# Start the Metrics Stream\n",
    "count_title_metrics = (streamingCountDF.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", checkpoint_title_path)\n",
    "    .start(output_title_path))\n",
    "\n",
    "count_bot_metrics = (streamingCountbotDF.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", checkpoint_bot_path)\n",
    "    .start(output_bot_path))\n",
    "\n",
    "\n",
    "# Start the Alert Stream\n",
    "query_alerts = (df_alerts.writeStream\n",
    "    .format(\"delta\") # Output as JSON to mimic a different system\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start(output_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a47720-9c1d-4127-a8a5-f70de3351f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display data\n",
    "print('Display of the title metrics (Count the number of edit per page)')\n",
    "display(spark.read.format(\"delta\").load(output_title_path))\n",
    "print('Display of the bot metrics (Count the number of bot edits)')\n",
    "display(spark.read.format(\"delta\").load(output_bot_path))\n",
    "print('Display of alert metric (See all bot edits)')\n",
    "display(spark.read.format(\"delta\").load(output_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91b95ae7-a750-4c70-aae3-ed66195a2468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the data folders are being created right now\n",
    "display(dbutils.fs.ls(output_title_path))\n",
    "# Check that the stream has 'consumed' the raw data\n",
    "display(dbutils.fs.ls(f\"{checkpoint_title_path}/offsets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8add96-92bf-44aa-bcdb-0b4d88fbc9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we are going to follow edit on wikimedia stream using the IMDb dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b383d2-153a-45b7-bf5b-c0d31e995a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Comedy series and movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67fa552-ea47-4e74-81c3-2240e6d5cd49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##--- Get Comedy titles from your IMDb data --##\n",
    "df_comedy = (df_basic\n",
    "    .filter(col(\"genres\").contains(\"Comedy\"))\n",
    "    .filter(col(\"startYear\")>2000)\n",
    "    .filter((col(\"titleType\")=='movie'))\\\n",
    "    .select(\"primaryTitle\")\\\n",
    "    .distinct().limit(100) # Take a good sample size\n",
    ")\n",
    "comedy_list = [row['primaryTitle'] for row in df_comedy.collect()]\n",
    "\n",
    "# Pre-process the keywords to replace spaces with a \"match space or underscore\" pattern\n",
    "cleaned_comedy_list = [rf\"\\b{k.replace(' ', '[ _]')}\\b\" for k in comedy_list]\n",
    "\n",
    "# Create the final pattern and makes it case-insensitive\n",
    "comedy_pattern = \"(?i)\" + \"|\".join(cleaned_comedy_list)\n",
    "\n",
    "# --- SIMPLE METRIC (Edit Counts) ---\n",
    "streamingCommedyCountsDF = (\n",
    "  streamingInputDF.filter(col(\"title\").rlike(comedy_pattern) & \n",
    "    ~col(\"title\").startswith(\"Wikipedia:\") & \n",
    "    ~col(\"title\").startswith(\"Category:\") &\n",
    "    ~col(\"title\").startswith(\"Talk:\") &\n",
    "    ~col(\"title\").startswith(\"User:\"))\\\n",
    "    .groupBy(\n",
    "      streamingInputDF.title, # group by edit made by bot boolean\n",
    "      window(\n",
    "        col(\"timestamp\").cast(\"timestamp\"), \n",
    "        \"10 minutes\"\n",
    "      )\n",
    "    )\n",
    "    .count())\n",
    "\n",
    "# temp volume for checkpoint storage\n",
    "volume = 'tmp_streamingDF'\n",
    "volume_path = f'/Volumes/{catalog}/{db_schema_checkpoints}/{volume}'\n",
    "volume_name = f'{catalog}.{db_schema_checkpoints}.{volume}'\n",
    "\n",
    "# drop old temp volume and recreate\n",
    "spark.sql(f\"DROP VOLUME IF EXISTS {volume_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {volume_name}\")\n",
    "\n",
    "\n",
    "# Define sub-paths\n",
    "data_output_path = f\"{volume_path}/data\"\n",
    "checkpoint_path = f\"{volume_path}/checkpoints\"\n",
    "\n",
    "# Update the writeStream\n",
    "streamingCommedyCountsDF.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start(data_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8e6378f-fbd0-4b3b-966b-f197263e18fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the data folders are being created right now\n",
    "display(dbutils.fs.ls(data_output_path))\n",
    "\n",
    "# Check that the stream has 'consumed' the raw data\n",
    "display(dbutils.fs.ls(f\"{checkpoint_path}/offsets\"))\n",
    "\n",
    "# Check if your Metrics actually saved data\n",
    "display(spark.read.format(\"delta\").load(data_output_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "180d8b95-0f94-46ce-b63e-d9fefe7233e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Verify if wikipedia pages exist in our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18471931-8781-492e-9d9e-58a0412d908b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the raw data statically for testing\n",
    "df_raw_test = spark.read.json(raw_data_path)\n",
    "\n",
    "# Check how many rows exist in the raw data total\n",
    "total_raw = df_raw_test.count()\n",
    "print(f\"Total rows in raw source: {total_raw}\")\n",
    "\n",
    "# Apply your exact filter logic\n",
    "df_test_matches = df_raw_test.filter(\n",
    "    col(\"title\").contains(comedy_pattern) & \n",
    "    ~col(\"title\").contains(\":\")\n",
    ")\n",
    "\n",
    "# 4. Show findings\n",
    "match_count = df_test_matches.count()\n",
    "print(f\"Total rows matching your Comedy pattern: {match_count}\")\n",
    "\n",
    "if match_count > 0:\n",
    "    print(\"Matches found! Sample titles:\")\n",
    "    df_test_matches.select(\"title\").distinct().show(5, False)\n",
    "else:\n",
    "    print(\"⚠️ No matches found. The filter might be too strict, or no edit was being made for those pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1bc46f-49e4-4d7f-92a4-9150cfc8b560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean-up Job\n",
    "\n",
    "print(f\"Scanning raw event volumes in schema: {catalog}.{uc_schema_raw_events}\")\n",
    "\n",
    "# 1. Get list of volumes\n",
    "volumes_df = spark.sql(f\"SHOW VOLUMES IN {catalog}.{uc_schema_raw_events}\")\n",
    "\n",
    "volumes_df.show()   \n",
    "\n",
    "# 2. Loop through returned volumes\n",
    "for row in volumes_df.collect():\n",
    "    volume_name = row['volume_name']   \n",
    "\n",
    "    if \"events_tmp\" in volume_name:\n",
    "        full_path = f\"/Volumes/{catalog}/{uc_schema_raw_events}/{volume_name}\"\n",
    "        print(f\"Deleting old raw volume: {full_path}\")\n",
    "        dbutils.fs.rm(full_path, recurse=True)\n",
    "\n",
    "print(\"Clean-up job finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b747c95-fd0d-4ec5-9284-dfd4a77fb05b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Project Documentation: Analysis & Observations ###\n",
    "\n",
    "While the streaming pipeline is fully functional and correctly configured to track the chosen entities (Drama, Star Wars, Johnny Depp, Game of Thrones, and Thriller but also entities collected from the IMDb dataset), the results in the final output tables may appear limited. This is due to a \"Low Activity\" phenomenon inherent to real-time event processing.\n",
    "\n",
    "**Technical Validation**\n",
    "\n",
    "The implementation was validated by:\n",
    "- Checking the _delta_log which confirms that the Delta transactions are committing successfully.\n",
    "- Verifying that the checkpointLocation is tracking offsets, proving the stream is \"listening\" for events even when none match the filters.\n",
    "- Routing \"Bot\" activity to separate files, demonstrating the functional logic of the Alerting System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f29d0c3-6ba4-4ce4-9aca-2ccb4cadf89e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa251ca-66cb-43dc-98fc-2e8f066dc8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Big_data_project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
